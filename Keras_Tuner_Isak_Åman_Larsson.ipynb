{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Tuner Isak Åman Larsson.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P9F4zkQQDnAz",
        "bSXbItNdE82k"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsakLarsson/Deep-Learning-labs/blob/main/Keras_Tuner_Isak_%C3%85man_Larsson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAUA6cvb_CNY"
      },
      "source": [
        "# Colab notebook to implement Fashion-MNIST Autotuned\n",
        "Created by Tomas Nordström, Umeå University. Runs model by Isak Åman Larsson\n",
        "\n",
        "This code uses [Keras Tuner](https://keras-team.github.io/keras-tuner/) for hyperparameter tuning.\n",
        "\n",
        "Based on\n",
        "* https://www.mikulskibartosz.name/using-keras-tuner-to-tune-hyperparameters-of-a-tensorflow-model/\n",
        "* https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html\n",
        "* https://pythonprogramming.net/keras-tuner-optimizing-neural-network-tutorial/\n",
        "* https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner\n",
        "* https://www.sicara.ai/blog/2019-14-07-determine-network-hyper-parameters-with-bayesian-optimization\n",
        "\n",
        "\n",
        "Revisions:\n",
        "*   2020-04-05 First revision /ToNo\n",
        "*   2020-11-15 Fixed printing of the n-best models. Adjusted some default parameters to get more stable results. /ToNo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-tp9iDDCpsz"
      },
      "source": [
        "## First we initilize our environment and run it within Tensorflow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUtSJt1C_0Xc"
      },
      "source": [
        "# Select Tensorflow 2.0\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1pcr2Em_Lzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5514cdd6-f3b5-453f-991b-bb82919581df"
      },
      "source": [
        "# Import needed libraries\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "import tensorflow.keras\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist \n",
        "print('Keras version:',tensorflow.keras.__version__)\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Matlab plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n",
            "Keras version: 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf0ZWo9JyEhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6294a283-db0d-456d-a933-a412353d7fe8"
      },
      "source": [
        "# Test for GPU and determine what GPU we have\n",
        "import sys\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "     print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "     IN_COLAB = 'google.colab' in sys.modules\n",
        "     if IN_COLAB:\n",
        "         print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "else:\n",
        "     !nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-73980e94-5e28-2608-3b68-855667e2a9ed)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKZs9sxX2fvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4627ba4b-d71a-41b3-8039-c7a2280ad903"
      },
      "source": [
        "# Get Keras Tuner\n",
        "import importlib\n",
        "try:\n",
        "    # try to import the module normally and put it in globals\n",
        "    globals()['kerastuner'] = importlib.import_module('kerastuner')\n",
        "except ImportError as e:\n",
        "    !git clone https://github.com/keras-team/keras-tuner.git\n",
        "    !pip install ./keras-tuner\n",
        "\n",
        "import kerastuner\n",
        "\n",
        "# Get some tuner search functions\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.tuners import Hyperband\n",
        "from kerastuner.tuners import BayesianOptimization\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-tuner'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 6714 (delta 8), reused 9 (delta 4), pack-reused 6693\u001b[K\n",
            "Receiving objects: 100% (6714/6714), 2.07 MiB | 23.79 MiB/s, done.\n",
            "Resolving deltas: 100% (4712/4712), done.\n",
            "Processing ./keras-tuner\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (20.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (1.18.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner==1.0.3) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner==1.0.3) (0.17.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.3-cp36-none-any.whl size=92439 sha256=ccf7e7879517c1cec3cf73ba8a51627bcbce75de73b62909bd19c213f882133d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/4e/d0/ed331a3363786e8a74848aa0589674f502cbb3f0321cdba844\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=9b3865a20f679fbc1065e1cc334090fd6ebf3bd9642ee43162bd34071dab9312\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.3 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhWTXQfXBiN6"
      },
      "source": [
        "## Get Data and Massage it into Shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ViGcOSI-_t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95caae48-767a-468d-f3e3-04ebd9f676df"
      },
      "source": [
        "# Get Fashion-MNIST training and test data from Keras database (https://keras.io/datasets/)\n",
        "(train_images0, train_labels0), (test_images, test_labels) = tensorflow.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Define labels\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Add empty color dimension (needed to run on TPU)\n",
        "train_images0 = np.expand_dims(train_images0, -1)\n",
        "test_images = np.expand_dims(test_images, -1)\n",
        "\n",
        "# Split the training set into a training and a validation set (20% is validation)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images0, train_labels0, test_size=0.20)\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "val_images = (val_images / 255) - 0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9F4zkQQDnAz"
      },
      "source": [
        "## Explore the data\n",
        "It is always advised to take a look at the data, to see if we need to massage it further. But this time we are not doing much of this as we should have done that in previous Lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_5rna7DDqBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe54c95-d132-425e-dd1b-59413da85a6b"
      },
      "source": [
        "# Print som baseic information of data set sizes and data sizes\n",
        "train_no,x,y,col = train_images.shape\n",
        "print('No training images:',train_no, ' with image size:',x,'x',y)\n",
        "label_no = len(train_labels)\n",
        "if (label_no != train_no) : \n",
        "  print('# labels do not match # training images')\n",
        "\n",
        "test_no,x,y,col = test_images.shape\n",
        "label_no = len(test_labels)\n",
        "print('No test images:',test_no)\n",
        "if (label_no != test_no) : \n",
        "  print('# labels do not match # test images')\n",
        "\n",
        "val_no,x,y,col = val_images.shape\n",
        "label_no = len(val_labels)\n",
        "print('No val images:',val_no)\n",
        "if (label_no != val_no) : \n",
        "  print('# labels do not match # val images')\n",
        "print('Training labels', np.unique(train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No training images: 48000  with image size: 28 x 28\n",
            "No test images: 10000\n",
            "No val images: 12000\n",
            "Training labels [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMYGkeRxJ__U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b8d463-6aca-4816-91ad-739f028dafeb"
      },
      "source": [
        "# As these are images (28x28) it can be interesting to plot some as images\n",
        "image_index = [42, 789] # \"Random\" images to print\n",
        "\n",
        "for index in image_index:\n",
        "  print( 'Label:', train_labels[index])\n",
        "  plt.figure()\n",
        "  plt.imshow(np.squeeze(train_images[index], axis=2))\n",
        "  plt.gray()\n",
        "  plt.grid(False)\n",
        "  plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ul3BdQM24CA"
      },
      "source": [
        "# Optimize model with Keras Tuner\n",
        "Here we test on of the hyperparameter optimizers called Keras Tuner. Documentation can be found here: https://keras-team.github.io/keras-tuner/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mExwzgrbAJi1"
      },
      "source": [
        "## Build a model structure with tunable hyperparameters \n",
        "hyperparameters are indicated with (hp.*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsqR_Xh21Sa"
      },
      "source": [
        "# This is a straight forward CNN model to decently solve Fashion MNIST\n",
        "# It can at least achieve 91.5% accuracy for me, with the parameters found:\n",
        "# conv_1_filter =  96, conv_1_kernel =   3, \n",
        "# conv_2_filter =  48, conv_2_kernel =   3\n",
        "# dense_1_units =  80\n",
        "# learning_rate = 0.001\n",
        "# But sometimes we only get 90.5%, this is after all a random search!\n",
        "\n",
        "def build_model_2Conv1Dense(hp):  \n",
        "  model = keras.Sequential([\n",
        "    # First Convolutional Layer\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_1_filter', min_value=64, max_value=128, step=16),\n",
        "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
        "        activation='relu',\n",
        "        input_shape=(28,28,1)\n",
        "    ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    # Second Convolutional Layer\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
        "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
        "        activation='relu'\n",
        "    ),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
        "\n",
        "    # A First Dense Layer\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(\n",
        "        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n",
        "        activation='relu'\n",
        "    ),\n",
        "\n",
        "    # A Final Dense Layer\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAT33optASBD"
      },
      "source": [
        "# Laboration: Kod för instrumentera din model att köras i Keras Tuner\n",
        "Jag tar min avskalade modell som jag i del 1 av denna lab tagit fram för att inte överträna så fruktansvärt mycket.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW-gld7XA8Pp"
      },
      "source": [
        "# Kompletera denna kod med din m odell\n",
        "def build_model_MyModel(hp):  \n",
        "  model = keras.Sequential([\n",
        "    # First Convolutional Layer\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_1_filter', min_value=64, max_value=128, step=16),\n",
        "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
        "        activation='relu',\n",
        "        input_shape=(28,28,1)\n",
        "    ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
        "\n",
        "    #Second convolutional layer\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
        "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
        "        activation='relu',\n",
        "    ),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
        "\n",
        "    # First Dense layer\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(\n",
        "        units=hp.Int('dense_1_units', min_value=32, max_value=786, step=16),\n",
        "        activation='relu'\n",
        "    ),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    # A Final Dense Layer\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "  ##### You can also try some other learning rates in the next line, or use another optimizer with other parameters\n",
        "  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-4])),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYVjX3YGjkz5"
      },
      "source": [
        "# Do a random search among the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXddQZv3PpBF"
      },
      "source": [
        "MAX_TRIALS = 20         # represents the number of hyperparameter combinations that will be tested by the tuner\n",
        "EXECUTION_PER_TRIAL = 2 # the number of models that should be built and fit for each trial for robustness purposes\n",
        "# To get better robustness we should do more than one execution per trail, \n",
        "# but for now we just want to explore the tool\n",
        "# If you have time more trails would be beneficial\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogrEip2G3VMq"
      },
      "source": [
        "# Random search\n",
        "if True:\n",
        "  tuner = RandomSearch(\n",
        "      build_model_MyModel, ##### Laboration: Change this to 'build_model_MyModel' #####\n",
        "      objective='val_accuracy',\n",
        "      max_trials=MAX_TRIALS,\n",
        "      executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "      directory='output',\n",
        "      project_name='FashionMNIST_R1') # This name need to be unique for a new tuning session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFgkueHO4c_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17985188-bdad-4e13-9108-bab2a5af2ea3"
      },
      "source": [
        "# Do the tuner search with RandomSearch as defined above\n",
        "if True:\n",
        "  batch_size = 128   ## Mini batch size, we will tune this later\n",
        "  noepochauto = 9    ## Number of epoch per hyper parameter testing\n",
        "  tuner.search(train_images, train_labels, \n",
        "              epochs=noepochauto, \n",
        "              validation_data=(val_images, val_labels), \n",
        "              batch_size=batch_size,\n",
        "              callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 20 Complete [00h 00m 52s]\n",
            "val_accuracy: 0.901666671037674\n",
            "\n",
            "Best val_accuracy So Far: 0.9131666719913483\n",
            "Total elapsed time: 00h 17m 47s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZijKaGllrp"
      },
      "source": [
        "## Now we have found a better set of hyper parameters\n",
        "Time to explore....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNzSDQPYlw3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02469391-0109-4333-a5e6-e2d141a9830d"
      },
      "source": [
        "# Get the best model so far\n",
        "model = tuner.get_best_models(num_models=1)[0]\n",
        "# Dump the best hyperparameters found\n",
        "vals = tuner.get_best_hyperparameters()[0].values\n",
        "for keys,values in vals.items():\n",
        "    print('%20s = %3.4g' %(keys,values))\n",
        "print()\n",
        "# Print the model sumary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       conv_1_filter =  96\n",
            "       conv_1_kernel =   3\n",
            "       conv_2_filter =  64\n",
            "       conv_2_kernel =   3\n",
            "       dense_1_units = 768\n",
            "       learning_rate = 0.0001\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 96)        960       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 26, 26, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        55360     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 768)               1229568   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                7690      \n",
            "=================================================================\n",
            "Total params: 1,294,218\n",
            "Trainable params: 1,293,898\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1FB79F_l20X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83dd6d4a-1164-448a-b8d1-6a369504853f"
      },
      "source": [
        "# Evaluate the model.\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\n",
        "print('Test accuracy: %.3f' % test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2718 - accuracy: 0.9060\n",
            "Test accuracy: 0.906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7rpNDpy-1yl"
      },
      "source": [
        "## Code to explore the n-best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVP7s9mwwFaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c62835f-a010-4a6f-d779-ad092e7fc566"
      },
      "source": [
        "# Let us dump the n-best\n",
        "\n",
        "if True:\n",
        "  explorenbest = 5\n",
        "\n",
        "  # Suppress warnings about optimizer state not being restored by tf.keras.\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  # Print a heading\n",
        "  vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[0].values\n",
        "  print('Test accuracy ', end = '')\n",
        "  for keys,values in vals.items():\n",
        "      print('%15s ' %(keys), end = '')\n",
        "  print('\\n')\n",
        "\n",
        "  # Now print each trail on a seperate row from best to worst\n",
        "  for ix in range(0,explorenbest): \n",
        "    # evaluate this trail\n",
        "    model = tuner.get_best_models(num_models=explorenbest)[ix]\n",
        "    test_loss, test_acc = model.evaluate(test_images,test_labels, verbose=0)\n",
        "    print('%15.3f ' % test_acc , end = '')\n",
        "    # get this trail's hyperparameters\n",
        "    vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[ix].values\n",
        "    for keys,values in vals.items():\n",
        "      print('%15.4g ' %(values), end = '')\n",
        "    # end this line and start the trail\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy   conv_1_filter   conv_1_kernel   conv_2_filter   conv_2_kernel   dense_1_units   learning_rate \n",
            "\n",
            "          0.906              96               3              64               3             768          0.0001 \n",
            "          0.902              96               3              64               3             320          0.0001 \n",
            "          0.902              80               3              32               5             720          0.0001 \n",
            "          0.901             112               5              48               5             592          0.0001 \n",
            "          0.903              96               5              48               5             320          0.0001 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-QCLbCCoS9"
      },
      "source": [
        "# Laboration: Analys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRnC8fKoDTUi"
      },
      "source": [
        "Gör en första analys av vilken optimerad model Keras Tuner har funnit, vilka andra hyperparametervärden fann optimeraren, och vilka prestanda den bästa modellen har. \n",
        "Om du tittar på de resulterande hyperparametrarna och jämför de med de gränser du satt på dem, ligger de på min/max-värdet för någon av dem, och vad skulle det betyda i så fall?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTUgxXYkDsl-"
      },
      "source": [
        "Först så ser jag att när jag tillät learning raten att gå ner till 0.0001 så verkar det ha varit det bästa valet eftersom att alla de bästa modellerna använde den learning raten. Alltså var jag korrekt i min undersökning under förra labben. Vidare verkar det första convolutional lagret föredra ca 100 noder och en 3,3 kernel och det andra verkar vilja ligga runt min satta maxgräns på 64 noder. Kanske bör man höja det omfånget ytterligare lite till. Det sista lagret verkar också föredra att ligga i de högre nodantalen för att få ett så bra resultat som möjligt. Jag ser att test lossen är rätt låg men jag hade önskat att accuracyn var högre då den just nu toppar på 90.6% vilket inte är lika bra som i min del 1 av denna labb. Men detta är ju bara random search så vi får se hur det går här nedanför med en lite mer riktad sökning av parametrar. Jag tror att random söka på detta sätt är bra för att få en hint om vad som kan vara bättre, parametrar som man kanske inte skulle testat annars helt enkelt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9v-8Y3MAie0"
      },
      "source": [
        "# Do a more directed search using BayesianOptimization\n",
        "\n",
        "Random search has its limitation and the tool have support for more adanced search strategies. Below we will use the BayesianOptimization strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of7WP3XSkUrd"
      },
      "source": [
        "# To get more stable results we are repeating the runs two times for each parameter setup\n",
        "MAX_TRIALS = 15         # represents the number of hyperparameter combinations that will be tested by the tuner\n",
        "EXECUTION_PER_TRIAL = 2 # the number of models that should be built and fit for each trial for robustness purposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us3knd0JREnv"
      },
      "source": [
        "# To have batch_size as a hyperparameter we need to define our own tuner\n",
        "# In this case, we are basing it on the BayesianOptimization tuner found in\n",
        "# https://keras-team.github.io/keras-tuner/documentation/tuners/#bayesianoptimization-class\n",
        "class MyTuner(kerastuner.tuners.BayesianOptimization):\n",
        "  def run_trial(self, trial, *args, **kwargs):\n",
        "    # You can add additional HyperParameters for preprocessing and custom training loops\n",
        "    # via overriding `run_trial`\n",
        "    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 32, 512, step=32, default=256)\n",
        "    super(MyTuner, self).run_trial(trial, *args, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laakQJumRcg8"
      },
      "source": [
        "# Bayesian search which also searches for batch_size\n",
        "tuner = MyTuner(\n",
        "    build_model_MyModel, ##### Laboration: Change this to 'build_model_MyModel' #####\n",
        "    max_trials=MAX_TRIALS,\n",
        "    objective='val_accuracy',\n",
        "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
        "    directory='output',\n",
        "    project_name='FashionMNIST_BX1' # This name need to be unique for a new tuning session\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6tCGAY3R94v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6566975c-040d-475e-ea23-0e1c61d23223"
      },
      "source": [
        "# Do the hyperparameter search\n",
        "noepochauto = 9   ## Max number of epochs per trail (but we have early stopping so this max is probably not reached)\n",
        "tuner.search(train_images, train_labels, \n",
        "             epochs=noepochauto, \n",
        "             validation_data=(val_images, val_labels), \n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 01m 41s]\n",
            "val_accuracy: 0.9122083485126495\n",
            "\n",
            "Best val_accuracy So Far: 0.914124995470047\n",
            "Total elapsed time: 00h 15m 06s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSxIn6ylBa1n"
      },
      "source": [
        "# Now we have a good set of hyper parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6-g5oJp52Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de57623f-fe53-4a19-e89d-81865c18870e"
      },
      "source": [
        "# Get the best model so far\n",
        "model = tuner.get_best_models(num_models=1)[0]\n",
        "# Dump the best hyperparameters found\n",
        "vals = tuner.get_best_hyperparameters(num_trials=5)[4].values\n",
        "for keys,values in vals.items():\n",
        "    print('%20s = %3.4g' %(keys,values))\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       conv_1_filter =  80\n",
            "       conv_1_kernel =   3\n",
            "       conv_2_filter =  32\n",
            "       conv_2_kernel =   3\n",
            "       dense_1_units = 768\n",
            "       learning_rate = 0.0001\n",
            "          batch_size =  64\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 128)       3328      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 10, 10, 32)        36896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 192)               153792    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1930      \n",
            "=================================================================\n",
            "Total params: 196,586\n",
            "Trainable params: 196,266\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMzl77Lb9EL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a49a25-ae6f-4255-d2f5-92a2867cb88b"
      },
      "source": [
        "# Evaluate the model.\n",
        "test_loss, test_acc = model.evaluate(\n",
        "  test_images,\n",
        "  test_labels\n",
        ")\n",
        "print('Test accuracy: %.3f' % test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2589 - accuracy: 0.9107\n",
            "Test accuracy: 0.911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSXbItNdE82k"
      },
      "source": [
        "## Code to explore the n-best model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4JunhBF0ZDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6c158c-c0d4-4a55-8484-2a9607ca349d"
      },
      "source": [
        "# Let us dump the n-best\n",
        "\n",
        "if True:\n",
        "  explorenbest = 5\n",
        "\n",
        "  # Suppress warnings about optimizer state not being restored by tf.keras.\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  # Print a heading\n",
        "  vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[0].values\n",
        "  print('Test accuracy ', end = '')\n",
        "  for keys,values in vals.items():\n",
        "      print('%15s ' %(keys), end = '')\n",
        "  print('\\n')\n",
        "\n",
        "  # Now print each trail on a seperate row from best to worst\n",
        "  for ix in range(0,explorenbest): \n",
        "    # evaluate this trail\n",
        "    model = tuner.get_best_models(num_models=explorenbest)[ix]\n",
        "    test_loss, test_acc = model.evaluate(test_images,test_labels, verbose=0)\n",
        "    print('%15.3f ' % test_acc , end = '')\n",
        "    # get this trail's hyperparameters\n",
        "    vals = tuner.get_best_hyperparameters(num_trials=explorenbest)[ix].values\n",
        "    for keys,values in vals.items():\n",
        "      print('%15.4g ' %(values), end = '')\n",
        "    # end this line and start the trail\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy   conv_1_filter   conv_1_kernel   conv_2_filter   conv_2_kernel   dense_1_units   learning_rate      batch_size \n",
            "\n",
            "          0.911             128               5              32               3             192          0.0001              32 \n",
            "          0.908              80               3              64               5             432          0.0001              32 \n",
            "          0.904              96               3              48               5             784          0.0001              64 \n",
            "          0.906              64               3              64               3             224          0.0001              64 \n",
            "          0.905              80               3              32               3             768          0.0001              64 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4C1nb-iXNXn"
      },
      "source": [
        "# With our found hyperparameters, continue training\n",
        "(Note that this probably is not be needed in this case.)\n",
        "\n",
        "However, for larger projects we might not have fully trained the models during hyperparameter search, and now need to do this fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63K8AY8456Ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de39e4ee-0e5a-47e1-aaf1-f51dc75c2654"
      },
      "source": [
        "# Fine-tune the model using the best parameters found as we might not be fully trained\n",
        "model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "epochs = 30      ## (max) number of epoch to run\n",
        "opt_batch_size = tuner.get_best_hyperparameters()[0]['batch_size']\n",
        "\n",
        "# Set callback functions to early stop training\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=2)]\n",
        "\n",
        "# Continue to train the model. (note that we are continuing from the training done during tuning)\n",
        "history = model.fit(\n",
        "  train_images, train_labels,\n",
        "  epochs=epochs,\n",
        "  batch_size=opt_batch_size,\n",
        "  verbose = 1,\n",
        "  validation_data=(val_images, val_labels),\n",
        "  # initial_epoch=noepochauto, ## how to get this number???? from... \"(root).optimizer.iter\" maybe\n",
        "  callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1706 - accuracy: 0.9370 - val_loss: 0.2406 - val_accuracy: 0.9122\n",
            "Epoch 2/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1603 - accuracy: 0.9418 - val_loss: 0.2655 - val_accuracy: 0.9087\n",
            "Epoch 3/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1465 - accuracy: 0.9476 - val_loss: 0.2514 - val_accuracy: 0.9113\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thog8ZJrCAGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b82817b-98b6-409e-c80b-c981376c7eca"
      },
      "source": [
        "# Evaluate the model.\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\n",
        "print('Test accuracy: %.3f' % test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2694 - accuracy: 0.9077\n",
            "Test accuracy: 0.908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9vaMFdJdP9H"
      },
      "source": [
        "# Do a clean training of the model\n",
        "An experiment to see how consistent the accuracy is when redoing the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arvHYdiJb9L7"
      },
      "source": [
        "# Code to reset model weights in TF2, from https://github.com/keras-team/keras/issues/341\n",
        "def reset_weights(model):\n",
        "  for layer in model.layers: \n",
        "    if isinstance(layer, tf.keras.Model):\n",
        "      reset_weights(layer)\n",
        "      continue\n",
        "    for k, initializer in layer.__dict__.items():\n",
        "      if \"initializer\" not in k:\n",
        "        continue\n",
        "      # find the corresponding variable\n",
        "      var = getattr(layer, k.replace(\"_initializer\", \"\"))\n",
        "      var.assign(initializer(var.shape, var.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVsCmCkzcEJe"
      },
      "source": [
        "# Reset the weights to some random values for a clean start\n",
        "reset_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQppiJTrcTUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74b19e0-6bcb-4c91-fac1-e701ceade3e1"
      },
      "source": [
        "# Train the network with optimized hyperparameters\n",
        "epochs = 30      ## (Maximum) number of epoch to run\n",
        "opt_batch_size = tuner.get_best_hyperparameters()[0]['batch_size'] # Get the optimal batch size\n",
        "\n",
        "# Set callback functions to early stop training\n",
        "callbacks = [tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=2)]\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(\n",
        "  train_images, train_labels,\n",
        "  epochs=epochs,\n",
        "  batch_size=opt_batch_size,\n",
        "  verbose = 1,\n",
        "  validation_data=(val_images, val_labels),\n",
        "  callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.5157 - accuracy: 0.8164 - val_loss: 0.3479 - val_accuracy: 0.8764\n",
            "Epoch 2/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3465 - accuracy: 0.8763 - val_loss: 0.3044 - val_accuracy: 0.8928\n",
            "Epoch 3/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2976 - accuracy: 0.8937 - val_loss: 0.2919 - val_accuracy: 0.8970\n",
            "Epoch 4/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2652 - accuracy: 0.9050 - val_loss: 0.2670 - val_accuracy: 0.9018\n",
            "Epoch 5/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2404 - accuracy: 0.9125 - val_loss: 0.2637 - val_accuracy: 0.9035\n",
            "Epoch 6/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2215 - accuracy: 0.9198 - val_loss: 0.2537 - val_accuracy: 0.9083\n",
            "Epoch 7/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2011 - accuracy: 0.9271 - val_loss: 0.2544 - val_accuracy: 0.9101\n",
            "Epoch 8/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1877 - accuracy: 0.9325 - val_loss: 0.2511 - val_accuracy: 0.9110\n",
            "Epoch 9/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1742 - accuracy: 0.9370 - val_loss: 0.2523 - val_accuracy: 0.9120\n",
            "Epoch 10/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1635 - accuracy: 0.9402 - val_loss: 0.2479 - val_accuracy: 0.9138\n",
            "Epoch 11/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1519 - accuracy: 0.9439 - val_loss: 0.2426 - val_accuracy: 0.9144\n",
            "Epoch 12/30\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1394 - accuracy: 0.9500 - val_loss: 0.2561 - val_accuracy: 0.9134\n",
            "Epoch 13/30\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1305 - accuracy: 0.9527 - val_loss: 0.2625 - val_accuracy: 0.9112\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsgyIXw4c6ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7166af7a-44e5-4404-8697-b14505fb5e20"
      },
      "source": [
        "# Evaluate the model.\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\n",
        "print('Test accuracy: %.3f' % test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.9062\n",
            "Test accuracy: 0.906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDaUgLoZFVVA"
      },
      "source": [
        "# Laboration: Analys av användning av Keras Tuner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-g6l86zFpcV"
      },
      "source": [
        "Gör en komplett analys av de optimerade modelerna du fått fram mha Keras Tuner.\n",
        "Vilka var dina hyperparametervärden, ursprungligen ifrån första labben, med slumpmässig sökning, med Bayesiask sökning. Vilka prestanda fick du för de tre fallen? Ändrade du sökgränserna efter den första slumpmässiga sökningen?\n",
        "Vilka hyperparametrar verkade vara mest betydelsfulla? Kan man se någon trend bland parametrarna (tex antalet faltningskärnor i tidiga respektive sena lager, etc)? Annat som du iaktagit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxhcYu7NG-N7"
      },
      "source": [
        "De parametrar som gavs av den slumpmässiga sökningen ville dra sig lite mer åt max-storleken på de olika lagren och en minsta storlek på faltningskärnorna. Gemensamn för de båda modellerna är att de verkar föredra en learning rate på 0.0001 då alla de bästa modellerna använde det. Jag ändrade till detta efter en första körning då jag sett tidigare att det verkar prestera mycket bättre. Random search modellen presterade dock bäst trots allt med en test accuracy på 91.1% jämfört med den andras 90.6%, vilket var lite av en besvikelse. Men jag är övertygad om att med lite mer tid och fipplande med modellen så kan ett bra resultat uppnås.\n",
        "\n",
        "    conv_1_filter =  96     RANDOM\n",
        "    conv_1_kernel =   3\n",
        "    conv_2_filter =  64\n",
        "    conv_2_kernel =   3\n",
        "    dense_1_units = 768\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "\n",
        "Den mer metodiska sökningen ledde till att det första lagret uppnådde sin maximala storlek både i antal noder och storlek på faltningskärnan. 128 respektive 5,5. Samt att det andra lagret gick till den minsta angivna storleken på 32 noder och 3,3 faltningskärna, vilket jag fann intressant. Det verkar trenda mot att ha större lager och kärna längst \"fram\" och att det blir mindre och mindre ju längre bak man kommer. Batch sizen var lite blandad bland de bästa modellerna men 32 låg i toppen. Trots all denna sökning så slog den inte den slumpmässigt valda modellen och kunde endast prestera 90.6% test accuracy. \n",
        "\n",
        "\n",
        "    conv_1_filter =  128     BAYEAN\n",
        "    conv_1_kernel =   5\n",
        "    conv_2_filter =  32\n",
        "    conv_2_kernel =   3\n",
        "    dense_1_units = 192\n",
        "    learning_rate = 0.0001\n",
        "    batch_size =  32\n",
        "\n",
        "\n",
        "De modeller som jag testat i del 1 av labben har vid tillfällen kommit upp i så högt som 91.7% men med betydligt högre loss (uppemot 40%) och det sker ej varje gång. Alltså skulle jag säga att det handlar mer om tur och slump, vilka noder som droppas osv. Däremot presterar min \"bästa\" modell med early stopping ganska jämnt på 90.7% vilket är ungefär lika bra som den framtagna modellen med bayeansk sökning. Jag tror dock att jag skulle valt en annan modell att undersöka på då denna redan från början haft väldigt lätt att överträna till datat. Jag kommer nog att göra det på fritiden utöver denna uppgift!. Avslutningsvis verkar det som ett väldigt bra verktyg för att optimisera modeller och jag tycker även att det belyser det faktum att det behövs enorma mängder datorkraft för att beräkna dessa saker. Man brute-forcar i stort sett en bra kombination av parametrar och det tar sin tid.\n",
        "\n",
        "    conv_1_filter =  64     MIN EGEN\n",
        "    conv_1_kernel =   5\n",
        "    conv_2_filter =  128\n",
        "    conv_2_kernel =   3\n",
        "    dense_1_units = 128\n",
        "    learning_rate = 0.0001\n",
        "    batch_size =  32\n",
        "\n"
      ]
    }
  ]
}